#!/usr/bin/env python3
"""Build daily TOP-N recommendation snapshots from optimized indicator logic.

This script reuses the latest best_logic.json generated by optimize_indicator_combo.py
and produces day-by-day TOP picks for a target date range.
"""

from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path

import numpy as np
import pandas as pd


DEFAULT_INPUT = Path("stock_data/korean_market_10y_with_indicators.parquet")
DEFAULT_LOGS_DIR = Path("logs")
DEFAULT_BEST_ROOT = Path("logs/indicator_combo_optimizer")
DEFAULT_KRX_MASTER = Path("data/krx_symbol_master.csv")
DEFAULT_START_DATE = "2026-01-01"


@dataclass
class BestLogic:
    feature_names: list[str]
    weights: np.ndarray
    source_path: Path


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Build daily recommendation TOP-N snapshots")
    parser.add_argument("--input", type=Path, default=DEFAULT_INPUT, help="Indicator parquet path")
    parser.add_argument(
        "--best-logic",
        type=Path,
        default=None,
        help="Path to best_logic.json (defaults to latest under logs/indicator_combo_optimizer/*/best_logic.json)",
    )
    parser.add_argument("--best-root", type=Path, default=DEFAULT_BEST_ROOT, help="Directory holding optimizer runs")
    parser.add_argument("--krx-master", type=Path, default=DEFAULT_KRX_MASTER, help="KRX symbol master CSV")
    parser.add_argument("--start-date", default=DEFAULT_START_DATE, help="Start date (YYYY-MM-DD)")
    parser.add_argument("--end-date", default=None, help="End date (YYYY-MM-DD). Default: latest available date")
    parser.add_argument("--top-n", type=int, default=5, help="Top N picks per day")
    parser.add_argument("--n-bins", type=int, default=20, help="Quantile bins per feature")
    parser.add_argument("--alpha", type=float, default=120.0, help="Bayesian smoothing strength")
    parser.add_argument(
        "--weight-turnover",
        type=float,
        default=0.45,
        help="Weight for daily trading value rank signal",
    )
    parser.add_argument(
        "--weight-ret1d",
        type=float,
        default=0.35,
        help="Weight for daily 1D return rank signal",
    )
    parser.add_argument(
        "--weight-model",
        type=float,
        default=0.20,
        help="Weight for original model probability rank signal",
    )
    parser.add_argument("--out", type=Path, default=None, help="Output CSV path")
    return parser.parse_args()


def find_latest_best_logic(best_root: Path) -> Path:
    if not best_root.exists():
        raise FileNotFoundError(f"Best logic root not found: {best_root}")

    candidates: list[Path] = []
    for run_dir in best_root.iterdir():
        if not run_dir.is_dir():
            continue
        best_path = run_dir / "best_logic.json"
        if best_path.exists():
            candidates.append(best_path)

    if not candidates:
        raise FileNotFoundError(f"No best_logic.json found under: {best_root}")

    return sorted(candidates, key=lambda p: p.parent.name, reverse=True)[0]


def load_best_logic(path: Path) -> BestLogic:
    payload = json.loads(path.read_text(encoding="utf-8"))
    best = payload.get("best", payload)

    feature_names = best.get("feature_names")
    weights = best.get("weights")
    if not isinstance(feature_names, list) or not isinstance(weights, list):
        raise ValueError(f"Invalid best logic format: {path}")
    if len(feature_names) != len(weights):
        raise ValueError(
            f"Feature/weight length mismatch ({len(feature_names)} != {len(weights)}) in {path}"
        )

    return BestLogic(
        feature_names=[str(x) for x in feature_names],
        weights=np.asarray(weights, dtype=np.float32),
        source_path=path,
    )


def fit_prob_mapper(
    x_train: np.ndarray,
    y_train: np.ndarray,
    n_bins: int,
    alpha: float,
) -> tuple[np.ndarray | None, np.ndarray, float]:
    valid = np.isfinite(x_train)
    if valid.sum() < 600:
        base = float(y_train.mean()) if y_train.size else 0.5
        return None, np.array([base], dtype=np.float32), base

    xv = x_train[valid].astype(np.float64, copy=False)
    yv = y_train[valid].astype(np.float64, copy=False)
    base = float(yv.mean()) if yv.size else 0.5

    q = np.linspace(0, 1, n_bins + 1)
    edges = np.quantile(xv, q)
    edges = np.unique(edges)

    if edges.size < 4:
        return None, np.array([base], dtype=np.float32), base

    inner = edges[1:-1]
    n_states = edges.size - 1
    bins = np.searchsorted(inner, xv, side="right")
    counts = np.bincount(bins, minlength=n_states).astype(np.float64)
    ups = np.bincount(bins, weights=yv, minlength=n_states).astype(np.float64)
    probs = (ups + alpha * base) / (counts + alpha)
    probs = np.clip(probs, 1e-4, 1 - 1e-4).astype(np.float32)
    return inner.astype(np.float32), probs, base


def apply_prob_mapper(
    x: np.ndarray,
    inner: np.ndarray | None,
    probs: np.ndarray,
    base: float,
) -> np.ndarray:
    out = np.full(x.shape[0], np.float32(base), dtype=np.float32)
    valid = np.isfinite(x)
    if not valid.any():
        return out

    if inner is None:
        out[valid] = probs[0]
        return out

    bins = np.searchsorted(inner, x[valid], side="right")
    bins = np.clip(bins, 0, probs.shape[0] - 1)
    out[valid] = probs[bins]
    return out


def to_scores(probs: np.ndarray) -> np.ndarray:
    if probs.size == 0:
        return np.array([], dtype=np.int32)
    p_min = float(np.min(probs))
    p_max = float(np.max(probs))
    if p_max <= p_min:
        return np.full(probs.shape[0], 85, dtype=np.int32)
    scaled = 70 + (probs - p_min) / (p_max - p_min) * 29
    return np.rint(np.clip(scaled, 0, 99)).astype(np.int32)


def normalize_weights(turnover: float, ret1d: float, model: float) -> tuple[float, float, float]:
    weights = np.array([turnover, ret1d, model], dtype=np.float64)
    if np.any(weights < 0):
        raise ValueError("All weights must be non-negative")
    total = float(weights.sum())
    if total <= 0:
        raise ValueError("At least one weight must be positive")
    normalized = weights / total
    return float(normalized[0]), float(normalized[1]), float(normalized[2])


def daily_percent_rank(series: pd.Series) -> pd.Series:
    filled = series.astype("float64").fillna(-np.inf)
    return filled.rank(pct=True, method="average")


def load_name_map(master_path: Path) -> dict[str, str]:
    if not master_path.exists():
        return {}
    master = pd.read_csv(master_path, usecols=["symbol", "name_ko"])
    master["symbol"] = master["symbol"].astype(str).str.upper()
    master["name_ko"] = master["name_ko"].astype(str).str.strip()
    return dict(zip(master["symbol"], master["name_ko"]))


def add_max_return_since_buy(top: pd.DataFrame, frame: pd.DataFrame) -> pd.DataFrame:
    if top.empty:
        top["max_return_since_buy"] = np.nan
        top["max_return_peak_date"] = ""
        return top

    high_ref = frame.loc[:, ["Ticker", "Date", "High"]].dropna(subset=["Ticker", "Date"])
    grouped: dict[str, tuple[np.ndarray, np.ndarray]] = {}
    for ticker, grp in high_ref.groupby("Ticker", observed=True, sort=False):
        dates = grp["Date"].to_numpy(dtype="datetime64[ns]")
        highs = grp["High"].to_numpy(dtype=np.float64, copy=False)
        grouped[str(ticker)] = (dates, highs)

    max_returns: list[float] = []
    peak_dates: list[str] = []
    for row in top.itertuples(index=False):
        ticker = str(row.Ticker)
        buy_date = np.datetime64(row.Date)
        buy_price = float(row.Close)
        series = grouped.get(ticker)

        if series is None or not np.isfinite(buy_price) or buy_price <= 0:
            max_returns.append(np.nan)
            peak_dates.append("")
            continue

        dates, highs = series
        start_idx = int(np.searchsorted(dates, buy_date, side="right"))
        if start_idx >= highs.shape[0]:
            max_returns.append(np.nan)
            peak_dates.append("")
            continue

        future_highs = highs[start_idx:]
        if future_highs.size == 0 or np.isnan(future_highs).all():
            max_returns.append(np.nan)
            peak_dates.append("")
            continue

        rel_idx = int(np.nanargmax(future_highs))
        peak_high = float(future_highs[rel_idx])
        peak_date = pd.Timestamp(dates[start_idx + rel_idx]).strftime("%Y-%m-%d")
        max_ret = (peak_high / buy_price) - 1.0

        max_returns.append(max_ret if np.isfinite(max_ret) else np.nan)
        peak_dates.append(peak_date)

    top = top.copy()
    top["max_return_since_buy"] = max_returns
    top["max_return_peak_date"] = peak_dates
    return top


def main() -> int:
    args = parse_args()

    if args.top_n <= 0:
        raise ValueError("--top-n must be positive")

    weight_turnover, weight_ret1d, weight_model = normalize_weights(
        args.weight_turnover, args.weight_ret1d, args.weight_model
    )

    best_logic_path = args.best_logic or find_latest_best_logic(args.best_root)
    best_logic = load_best_logic(best_logic_path)

    if not args.input.exists():
        raise FileNotFoundError(f"Input parquet not found: {args.input}")

    required_cols = ["Date", "Ticker", "Close", "High", "Volume", *best_logic.feature_names]
    frame = pd.read_parquet(args.input, columns=required_cols)
    frame["Date"] = pd.to_datetime(frame["Date"]).dt.tz_localize(None)
    frame["Ticker"] = frame["Ticker"].astype("string")

    frame = frame.sort_values(["Ticker", "Date"], kind="mergesort").reset_index(drop=True)
    frame["NextClose"] = frame.groupby("Ticker", observed=True, sort=False)["Close"].shift(-1)
    frame["PrevClose"] = frame.groupby("Ticker", observed=True, sort=False)["Close"].shift(1)

    labeled_mask = frame["NextClose"].notna() & frame["Close"].gt(0)
    train = frame.loc[labeled_mask].copy()
    train["TargetUp"] = (train["NextClose"] > train["Close"]).astype(np.uint8)
    y_train = train["TargetUp"].to_numpy(dtype=np.uint8, copy=False)

    mappers: dict[str, tuple[np.ndarray | None, np.ndarray, float]] = {}
    for col in best_logic.feature_names:
        x = train[col].to_numpy(dtype=np.float32, copy=False)
        mappers[col] = fit_prob_mapper(x, y_train, n_bins=args.n_bins, alpha=args.alpha)

    start_ts = pd.Timestamp(args.start_date)
    end_ts = pd.Timestamp(args.end_date) if args.end_date else frame["Date"].max()

    infer = frame.loc[
        (frame["Date"] >= start_ts) & (frame["Date"] <= end_ts),
        ["Date", "Ticker", "Close", "Volume", "PrevClose", *best_logic.feature_names],
    ].copy()
    probs = np.zeros(len(infer), dtype=np.float32)
    for weight, col in zip(best_logic.weights, best_logic.feature_names):
        inner, p_bins, base = mappers[col]
        x = infer[col].to_numpy(dtype=np.float32, copy=False)
        probs += weight * apply_prob_mapper(x, inner=inner, probs=p_bins, base=base)

    probs = np.clip(probs, 1e-4, 1 - 1e-4)
    infer["trading_value"] = infer["Close"].astype("float64") * infer["Volume"].astype("float64")
    infer["ret_1d"] = np.where(
        infer["PrevClose"].astype("float64") > 0,
        infer["Close"].astype("float64") / infer["PrevClose"].astype("float64") - 1.0,
        np.nan,
    )
    infer["prob_up_next_day"] = probs
    infer["turnover_rank_pct"] = infer.groupby("Date", observed=True)["trading_value"].transform(daily_percent_rank)
    infer["ret1d_rank_pct"] = infer.groupby("Date", observed=True)["ret_1d"].transform(daily_percent_rank)
    infer["model_rank_pct"] = infer.groupby("Date", observed=True)["prob_up_next_day"].transform(daily_percent_rank)
    infer["score_turnover"] = weight_turnover * infer["turnover_rank_pct"]
    infer["score_ret1d"] = weight_ret1d * infer["ret1d_rank_pct"]
    infer["score_model"] = weight_model * infer["model_rank_pct"]
    infer["final_score"] = (
        infer["score_turnover"] + infer["score_ret1d"] + infer["score_model"]
    )
    infer["ai_score"] = to_scores(probs)
    infer = infer.sort_values(
        ["Date", "final_score", "prob_up_next_day", "Ticker"],
        ascending=[True, False, False, True],
        kind="mergesort",
    )
    infer["rank"] = infer.groupby("Date", observed=True).cumcount() + 1

    top = infer.loc[
        infer["rank"] <= args.top_n,
        [
            "Date",
            "rank",
            "Ticker",
            "Close",
            "trading_value",
            "ret_1d",
            "prob_up_next_day",
            "turnover_rank_pct",
            "ret1d_rank_pct",
            "model_rank_pct",
            "score_turnover",
            "score_ret1d",
            "score_model",
            "final_score",
            "ai_score",
        ],
    ].copy()
    top = add_max_return_since_buy(top, frame)

    name_map = load_name_map(args.krx_master)
    top["name"] = top["Ticker"].map(name_map).fillna(top["Ticker"])
    top["Date"] = top["Date"].dt.strftime("%Y-%m-%d")
    top = top[
        [
            "Date",
            "rank",
            "Ticker",
            "name",
            "Close",
            "trading_value",
            "ret_1d",
            "prob_up_next_day",
            "turnover_rank_pct",
            "ret1d_rank_pct",
            "model_rank_pct",
            "score_turnover",
            "score_ret1d",
            "score_model",
            "final_score",
            "ai_score",
            "max_return_since_buy",
            "max_return_peak_date",
        ]
    ]

    max_date = top["Date"].max() if not top.empty else start_ts.strftime("%Y-%m-%d")
    if args.out is not None:
        out_path = args.out
    else:
        out_path = DEFAULT_LOGS_DIR / f"daily_top5_recommendations_{args.start_date.replace('-', '')}_to_{max_date.replace('-', '')}.csv"

    out_path.parent.mkdir(parents=True, exist_ok=True)
    top.to_csv(out_path, index=False, encoding="utf-8-sig")

    print(f"best_logic: {best_logic.source_path}")
    print(f"output_csv: {out_path}")
    print(f"date_range: {top['Date'].min() if not top.empty else '-'} ~ {top['Date'].max() if not top.empty else '-'}")
    print(f"trading_days: {top['Date'].nunique()}")
    print(f"rows: {len(top)}")
    print(
        "weights:"
        f" turnover={weight_turnover:.3f},"
        f" ret1d={weight_ret1d:.3f},"
        f" model={weight_model:.3f}"
    )
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
